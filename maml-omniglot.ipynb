{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Definition\n",
    "\n",
    "`k_shot` is the number of samples for each class used in support set.\n",
    "\n",
    "`q_query` is the number of samples for each class used in query set, the total number in query set is $\\text{q\\_query} \\times \\text{n\\_way}$. And we have.\n",
    "$$\n",
    "\\text{k\\_shot} + \\text{q\\_query} \\le \\text{number of samples for each class}\n",
    "$$\n",
    "\n",
    "`meta_batch_size` is the number of tasks in each meta-training/meta-testing iteration.\n",
    "\n",
    "`num_iterations` is the number of total meta-training iterations, each meta-training iteration updates the model initialization parameters once.\n",
    "\n",
    "`display_gap` is the step size to visualize the meta-training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = './data/Omniglot/images_background/'\n",
    "test_data_path = './data/Omniglot/images_evaluation/'\n",
    "n_way = 5\n",
    "k_shot = 5\n",
    "q_query = 5\n",
    "outer_lr = 0.001\n",
    "inner_lr = 0.04\n",
    "meta_batch_size = 32\n",
    "train_inner_step = 1\n",
    "eval_inner_step = 3\n",
    "num_iterations = 1000\n",
    "num_workers = 0\n",
    "valid_size = 0.2\n",
    "random_seed = 42\n",
    "display_gap = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Importation & Random Seed Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Definition\n",
    "\n",
    "`MAMLDataset` is a general dataset framework for MAML, which leaves two functions to implement for each specified dataset. \n",
    "\n",
    "Since the tasks are randomly sampled from the entire class dataset each time, `__len__` basically does not influence the results.\n",
    "This means that the size of it can be seen as Infinity. Here, we set it as the number of all classes, which is helpful in spilting.\n",
    "\n",
    "`OmniglotDataset` is a implemented dataset class for Omniglot dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAMLDataset(Dataset):\n",
    "    def __init__(self, data_path, transform, n_way=5, k_shot=1, q_query=1):\n",
    "\n",
    "        self.file_list = self.get_file_list(data_path)\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.q_query = q_query\n",
    "        self.transform = transform\n",
    "\n",
    "    def get_file_list(self, data_path):\n",
    "        raise NotImplementedError('get_file_list function not implemented!')\n",
    "\n",
    "    def get_one_task_data(self):\n",
    "        raise NotImplementedError('get_one_task_data function not implemented!')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_one_task_data()\n",
    "\n",
    "\n",
    "class OmniglotDataset(MAMLDataset):\n",
    "    def get_file_list(self, data_path):\n",
    "        \"\"\"\n",
    "        Get a list of all classes.\n",
    "        Args:\n",
    "            data_path: Omniglot data path\n",
    "\n",
    "        Returns: list of all classes\n",
    "\n",
    "        \"\"\"\n",
    "        return [f for f in glob.glob(data_path + \"**/character*\", recursive=True)]\n",
    "\n",
    "    def get_one_task_data(self):\n",
    "        \"\"\"\n",
    "        Get ones task maml data, include one batch support images and labels, one batch query images and labels.\n",
    "        Returns: support_data, query_data\n",
    "\n",
    "        \"\"\"\n",
    "        img_dirs = random.sample(self.file_list, self.n_way)\n",
    "        support_data = []\n",
    "        query_data = []\n",
    "\n",
    "        support_image = []\n",
    "        support_label = []\n",
    "        query_image = []\n",
    "        query_label = []\n",
    "\n",
    "        for label, img_dir in enumerate(img_dirs):\n",
    "            img_list = [f for f in glob.glob(img_dir + \"**/*.png\", recursive=True)]\n",
    "            images = random.sample(img_list, self.k_shot + self.q_query)\n",
    "\n",
    "            # Read support set\n",
    "            for img_path in images[:self.k_shot]:\n",
    "                image = self.transform(Image.open(img_path))\n",
    "                image = np.array(image)\n",
    "                support_data.append((image, label))\n",
    "\n",
    "            # Read query set\n",
    "            for img_path in images[self.k_shot:]:\n",
    "                image = self.transform(Image.open(img_path))\n",
    "                image = np.array(image)\n",
    "                query_data.append((image, label))\n",
    "\n",
    "        # shuffle support set\n",
    "        random.shuffle(support_data)\n",
    "        for data in support_data:\n",
    "            support_image.append(data[0])\n",
    "            support_label.append(data[1])\n",
    "\n",
    "        # shuffle query set\n",
    "        random.shuffle(query_data)\n",
    "        for data in query_data:\n",
    "            query_image.append(data[0])\n",
    "            query_label.append(data[1])\n",
    "\n",
    "        return np.array(support_image), np.array(support_label), np.array(query_image), np.array(query_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Model Definition\n",
    "\n",
    "This defines the base classifer for MAML, which can be replaced by any gradient-based classifier. But it is important to note that, one special function `functional_forward` should be defined according to the property of MAML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv2d = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def ConvBlockFunction(input, w, b, w_bn, b_bn):\n",
    "    x = F.conv2d(input, w, b, padding=1)\n",
    "    x = F.batch_norm(x, running_mean=None, running_var=None, weight=w_bn, bias=b_bn, training=True)\n",
    "    x = F.relu(x)\n",
    "    output = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_ch, n_way):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = ConvBlock(in_ch, 64)\n",
    "        self.conv2 = ConvBlock(64, 64)\n",
    "        self.conv3 = ConvBlock(64, 64)\n",
    "        self.conv4 = ConvBlock(64, 64)\n",
    "        self.logits = nn.Linear(64, n_way)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "    def functional_forward(self, x, params):\n",
    "        for block in [1, 2, 3, 4]:\n",
    "            x = ConvBlockFunction(\n",
    "                x,\n",
    "                params[f\"conv{block}.conv2d.weight\"],\n",
    "                params[f\"conv{block}.conv2d.bias\"],\n",
    "                params.get(f\"conv{block}.bn.weight\"),\n",
    "                params.get(f\"conv{block}.bn.bias\"),\n",
    "            )\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.linear(x, params[\"logits.weight\"], params[\"logits.bias\"])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions (1): get dataset & spilt train and valid\n",
    "\n",
    "`train_transform` and `test_transform` is used for data augmentation. You can change this in your implementation as you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(size=28),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_trasfrom = transforms.Compose([\n",
    "    transforms.Resize(size=28),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def get_dataset(\n",
    "        train_data_path,\n",
    "        test_data_path,\n",
    "        n_way,\n",
    "        k_shot,\n",
    "        q_query\n",
    "):\n",
    "    \"\"\"\n",
    "    Get maml dataset.\n",
    "    Args:\n",
    "        args: ArgumentParser\n",
    "\n",
    "    Returns: dataset\n",
    "    \"\"\"\n",
    "    train_dataset = OmniglotDataset(train_data_path, \n",
    "                                    train_transform,\n",
    "                                    n_way, \n",
    "                                    k_shot, \n",
    "                                    q_query)\n",
    "    \n",
    "    valid_dataset = OmniglotDataset(train_data_path, \n",
    "                                    test_trasfrom,\n",
    "                                    n_way, \n",
    "                                    k_shot, \n",
    "                                    q_query)\n",
    "\n",
    "    test_dataset = OmniglotDataset(test_data_path, \n",
    "                                   test_trasfrom,\n",
    "                                   n_way, \n",
    "                                   k_shot, \n",
    "                                   q_query)\n",
    "    \n",
    "    train_dataset, valid_dataset = spilt_train_valid(train_dataset, \n",
    "                                                     valid_dataset, \n",
    "                                                     valid_size)\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "\n",
    "def spilt_train_valid(train_dataset, valid_dataset, valid_set_size):\n",
    "    \"\"\"\n",
    "    Spilt train dataset into train and valid dataset according to the given size.\n",
    "    Args:\n",
    "        train_dataset: original train dataset\n",
    "        valid_dataset: valid dataset to put into\n",
    "        valid_set_size: given size in terms of proportion\n",
    "    \n",
    "    Returns: spilted train and valid datasets\n",
    "    \"\"\"\n",
    "    valid_set_size = int(valid_set_size * len(train_dataset))\n",
    "    train_set_size = len(train_dataset) - valid_set_size\n",
    "\n",
    "    file_list = train_dataset.file_list\n",
    "    random.shuffle(file_list)\n",
    "    \n",
    "    train_dataset.file_list = file_list[:train_set_size]\n",
    "    valid_dataset.file_list = file_list[train_set_size:]\n",
    "\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function (2): train one meta-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maml_train(model, \n",
    "               support_images,\n",
    "               support_labels,\n",
    "               query_images,\n",
    "               query_labels, \n",
    "               inner_step, \n",
    "               inner_lr,\n",
    "               optimizer, \n",
    "               loss_fn,\n",
    "               is_train=True):\n",
    "    \"\"\"\n",
    "    Train the model using MAML method.\n",
    "    Args:\n",
    "        model: Any model\n",
    "        support_images: several task support images\n",
    "        support_labels: several  support labels\n",
    "        query_images: several query images\n",
    "        query_labels: several query labels\n",
    "        inner_step: support data training step\n",
    "        inner_lr: inner\n",
    "        optimizer: optimizer\n",
    "        is_train: whether train\n",
    "\n",
    "    Returns: meta loss, meta accuracy\n",
    "    \"\"\"\n",
    "    meta_loss = []\n",
    "    meta_acc = []\n",
    "\n",
    "    # Get support set and query set data for one train task\n",
    "    for support_image, support_label, query_image, query_label \\\n",
    "        in zip(support_images, support_labels, query_images, query_labels):\n",
    "\n",
    "        fast_weights = collections.OrderedDict(model.named_parameters())\n",
    "        for _ in range(inner_step):\n",
    "            # Update weight\n",
    "            support_logit = model.functional_forward(support_image, fast_weights)\n",
    "            support_loss = loss_fn(support_logit, support_label)\n",
    "            grads = torch.autograd.grad(support_loss, \n",
    "                                        fast_weights.values(), \n",
    "                                        create_graph=True)\n",
    "            fast_weights = collections.OrderedDict((name, param - inner_lr * grads)\n",
    "                                                   for ((name, param), grads) \n",
    "                                                   in zip(fast_weights.items(), grads))\n",
    "\n",
    "        # Use trained weight to get query loss\n",
    "        query_logit = model.functional_forward(query_image, fast_weights)\n",
    "        query_prediction = torch.max(query_logit, dim=1)[1]\n",
    "\n",
    "        query_loss = loss_fn(query_logit, query_label)\n",
    "        query_acc = torch.eq(query_label, query_prediction).sum() / len(query_label)\n",
    "\n",
    "        meta_loss.append(query_loss)\n",
    "        meta_acc.append(query_acc.data.cpu().numpy())\n",
    "\n",
    "    meta_loss = torch.stack(meta_loss).mean()\n",
    "    meta_acc = np.mean(meta_acc)\n",
    "\n",
    "    if is_train:\n",
    "        optimizer.zero_grad()\n",
    "        meta_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return meta_loss, meta_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tasks, valid_tasks, test_tasks = get_dataset(train_data_path,\n",
    "                                                   test_data_path,\n",
    "                                                   n_way,\n",
    "                                                   k_shot,\n",
    "                                                   q_query)\n",
    "\n",
    "train_loader = DataLoader(train_tasks, batch_size=meta_batch_size, \n",
    "                            shuffle=True, drop_last=True,  num_workers=num_workers)\n",
    "\n",
    "valid_loader = DataLoader(valid_tasks, batch_size=meta_batch_size, \n",
    "                            shuffle=True, drop_last=True, num_workers=num_workers)\n",
    "\n",
    "test_loader = DataLoader(test_tasks, batch_size=meta_batch_size, \n",
    "                            shuffle=False, drop_last=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model & Optimizer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(in_ch=1, n_way=n_way)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), outer_lr)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAML Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_best_acc = 0\n",
    "train_acc = []\n",
    "valid_acc = []\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "train_iter = iter(train_loader)\n",
    "valid_iter = iter(valid_loader)\n",
    "\n",
    "for iteration in range(1, num_iterations+1):\n",
    "\n",
    "    # ========================= train model =====================\n",
    "    model.train()\n",
    "    try:\n",
    "        support_images, support_labels, query_images, query_labels = next(train_iter)\n",
    "    except StopIteration:\n",
    "        train_iter = iter(train_loader)\n",
    "        support_images, support_labels, query_images, query_labels = next(train_iter)\n",
    "\n",
    "    # Get support set and query set data for one meta-batch (several tasks)\n",
    "    support_images = support_images.float().to(device)\n",
    "    support_labels = support_labels.long().to(device)\n",
    "    query_images = query_images.float().to(device)\n",
    "    query_labels = query_labels.long().to(device)\n",
    "\n",
    "    # Train init-paras on one meta-batch and get the corresponding \n",
    "    # average evaluation (query) loss and acc among these training tasks\n",
    "    loss, acc = maml_train(model, \n",
    "                           support_images, \n",
    "                           support_labels, \n",
    "                           query_images, \n",
    "                           query_labels,\n",
    "                           train_inner_step, \n",
    "                           inner_lr,\n",
    "                           optimizer, \n",
    "                           loss_fn, \n",
    "                           is_train=True)\n",
    "    \n",
    "    train_loss.append(loss.item())\n",
    "    train_acc.append(acc)\n",
    "\n",
    "    if iteration == 1 or iteration % display_gap == 0:\n",
    "        print('======================== Iteration: {} ========================'.format(iteration))\n",
    "        print('Meta Train Loss: {:.3f}, Meta Train Acc: {:.2f}%'.format(loss, 100 * acc))\n",
    "    \n",
    "    # ====================== validate model ====================\n",
    "    model.eval()\n",
    "    try:\n",
    "        support_images, support_labels, query_images, query_labels = next(valid_iter)\n",
    "    except StopIteration:\n",
    "        valid_iter = iter(valid_loader)\n",
    "        support_images, support_labels, query_images, query_labels = next(valid_iter)\n",
    "    \n",
    "    support_images = support_images.float().to(device)\n",
    "    support_labels = support_labels.long().to(device)\n",
    "    query_images = query_images.float().to(device)\n",
    "    query_labels = query_labels.long().to(device)\n",
    "\n",
    "    loss, acc = maml_train(model, \n",
    "                            support_images, \n",
    "                            support_labels, \n",
    "                            query_images, \n",
    "                            query_labels,\n",
    "                            train_inner_step, \n",
    "                            inner_lr,\n",
    "                            optimizer, \n",
    "                            loss_fn, \n",
    "                            is_train=False)\n",
    "\n",
    "    valid_loss.append(loss.item())\n",
    "    valid_acc.append(acc)\n",
    "\n",
    "    if iteration == 1 or iteration % display_gap == 0:\n",
    "        print('Meta Valid Loss: {:.3f}, Meta Valid Acc: {:.2f}%'.format(loss, 100 * acc))\n",
    "        print('=============================================================='.format(iteration))\n",
    "\n",
    "        # ========================= plot ==========================\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(121)\n",
    "        plt.plot(train_acc, '-o', label=\"train acc\")\n",
    "        plt.plot(valid_acc, '-o', label=\"valid acc\")\n",
    "        plt.xlabel('Trainin iteration')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(\"Accuracy Curve by Iteration\")\n",
    "        plt.legend()\n",
    "        plt.subplot(122)\n",
    "        plt.plot(train_loss, '-o', label=\"train loss\")\n",
    "        plt.plot(valid_loss, '-o', label=\"valid loss\")\n",
    "        plt.xlabel('Trainin iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(\"Loss Curve by Iteration\")\n",
    "        plt.legend()\n",
    "        plt.suptitle(\"Omniglot-{}way-{}shot\".format(n_way, k_shot))\n",
    "        plt.show()\n",
    "\n",
    "    # ========================= save model =====================\n",
    "    if np.mean(acc) > valid_best_acc:\n",
    "        print('Validation accuracy improved ({:.2f}% --> {:.2f}%).'.format(100 * valid_best_acc, 100 * acc))\n",
    "        valid_best_acc = np.mean(acc)\n",
    "        torch.save(model.state_dict(), 'maml-para.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== evaluate model ====================\n",
    "model.load_state_dict(torch.load('maml-para.pt'))\n",
    "test_acc = []\n",
    "test_loss = []\n",
    "\n",
    "test_bar = tqdm(test_loader)\n",
    "model.eval()\n",
    "for support_images, support_labels, query_images, query_labels in test_bar:\n",
    "    test_bar.set_description(\"Testing\")\n",
    "\n",
    "    support_images = support_images.float().to(device)\n",
    "    support_labels = support_labels.long().to(device)\n",
    "    query_images = query_images.float().to(device)\n",
    "    query_labels = query_labels.long().to(device)\n",
    "\n",
    "    loss, acc = maml_train(model, \n",
    "                           support_images, \n",
    "                           support_labels, \n",
    "                           query_images, \n",
    "                           query_labels,\n",
    "                           eval_inner_step, \n",
    "                           inner_lr,\n",
    "                           optimizer, \n",
    "                           loss_fn, \n",
    "                           is_train=False)\n",
    "    test_loss.append(loss.item())\n",
    "    test_acc.append(acc)\n",
    "\n",
    "test_loss = np.mean(test_loss)\n",
    "test_acc = np.mean(test_acc)\n",
    "print('Meta Test Loss: {:.3f}, Meta Test Acc: {:.2f}%'.format(test_loss, 100 * test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
